{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kq0yfz4cogxb",
        "outputId": "c1df3e0c-e9f7-41f3-e432-8b271495c6b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m115.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q python-dotenv evaluate torch transformers sentence-transformers faiss-cpu nltk gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score bert_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae_idZlGq-bz",
        "outputId": "8c875991-c640-463f-8ec5-15dedf4c3cfa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bert_score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.6.0+cu124)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.52.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert_score) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.33.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.2.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2025.6.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=3.0.0->bert_score) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
            "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=c7aaf4ae16df94f97d173168122d00b918025684c00792e81cbea93ad82774d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score, bert_score\n",
            "Successfully installed bert_score-0.3.13 rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import requests\n",
        "import evaluate\n",
        "from torch import nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.display import display, Markdown\n",
        "from dotenv import load_dotenv\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Initialize environment\n",
        "load_dotenv()\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Hugging Face login if token exists\n",
        "if os.getenv(\"HF_TOKEN\"):\n",
        "    login(token=os.getenv(\"HF_TOKEN\"))\n",
        "\n",
        "class LCMConfig:\n",
        "    def __init__(self):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.embd_dim = 768\n",
        "        self.dim = 768\n",
        "        self.layers = 6\n",
        "        self.heads = 12\n",
        "        self.dropout = 0.05\n",
        "        self.chunk_size = 768\n",
        "        self.summary_length = 250\n",
        "        self.num_key_concepts = 7\n",
        "        self.segmenter_model = \"bert-large-uncased\"\n",
        "        self.embedding_model = \"all-mpnet-base-v2\"\n",
        "        self.temperature = 0.5\n",
        "        self.num_beams = 4\n",
        "        self.arxiv_api_url = \"http://export.arxiv.org/api/query?\"\n",
        "        self.max_retrieved_papers = int(os.getenv(\"ARXIV_MAX_RESULTS\", 5))\n",
        "        self.lora_rank = 8\n",
        "        self.lora_alpha = 16\n",
        "        self.lora_dropout = 0.1\n",
        "        self.model_cache_dir = os.getenv(\"MODEL_CACHE_DIR\", \"./models\")\n",
        "\n",
        "# [Rest of your classes and functions remain exactly the same...]\n",
        "# DocumentProcessor, ConceptSelector, FAISSRetriever, ArXivSearch, RAGSummarizer\n",
        "# evaluate_summaries, display_results, run_search_pipeline\n",
        "\n",
        "\n",
        "\n",
        "class DocumentProcessor:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config.segmenter_model)\n",
        "\n",
        "    def chunk_document(self, text):\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        chunks, current_chunk = [], []\n",
        "        current_length = 0\n",
        "\n",
        "        for sent in sentences:\n",
        "            sent_tokens = self.tokenizer.tokenize(sent)\n",
        "            if current_length + len(sent_tokens) > self.config.chunk_size and current_chunk:\n",
        "                chunks.append(\" \".join(current_chunk))\n",
        "                current_chunk = []\n",
        "                current_length = 0\n",
        "\n",
        "            current_chunk.append(sent)\n",
        "            current_length += len(sent_tokens)\n",
        "\n",
        "            if current_length > self.config.chunk_size * 0.75:\n",
        "                chunks.append(\" \".join(current_chunk))\n",
        "                current_chunk = []\n",
        "                current_length = 0\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "class ConceptSelector(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.attention = nn.MultiheadAttention(dim, num_heads=4, batch_first=True)\n",
        "        self.scorer = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, dim * 2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim * 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        attn_out, _ = self.attention(embeddings, embeddings, embeddings)\n",
        "        return self.scorer(attn_out).squeeze(-1)\n",
        "\n",
        "class FAISSRetriever:\n",
        "    def __init__(self, dim):\n",
        "        self.index = faiss.IndexFlatIP(dim)\n",
        "        self.metadata = []\n",
        "\n",
        "    def add(self, embedding, data):\n",
        "        if not isinstance(embedding, np.ndarray):\n",
        "            embedding = np.array(embedding, dtype=np.float32)\n",
        "        self.index.add(embedding.reshape(1, -1))\n",
        "        self.metadata.append(data)\n",
        "\n",
        "    def search(self, query_embedding, k=5):\n",
        "        if not isinstance(query_embedding, np.ndarray):\n",
        "            query_embedding = np.array(query_embedding, dtype=np.float32)\n",
        "        distances, indices = self.index.search(query_embedding.reshape(1, -1), k)\n",
        "        return [self.metadata[i] for i in indices[0]], distances[0]\n",
        "\n",
        "class ArXivSearch:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.embedding_model = SentenceTransformer(config.embedding_model, device=config.device)\n",
        "\n",
        "    def search_papers(self, query, max_results=5):\n",
        "        \"\"\"Search ArXiv papers using their API\"\"\"\n",
        "        params = {\n",
        "            \"search_query\": f\"ti:{query}\",\n",
        "            \"start\": 0,\n",
        "            \"max_results\": max_results,\n",
        "            \"sortBy\": \"relevance\",\n",
        "            \"sortOrder\": \"descending\"\n",
        "        }\n",
        "\n",
        "        response = requests.get(self.config.arxiv_api_url, params=params)\n",
        "        if response.status_code != 200:\n",
        "            raise Exception(f\"ArXiv API request failed with status {response.status_code}\")\n",
        "\n",
        "        return self._parse_arxiv_response(response.text)\n",
        "\n",
        "    def _parse_arxiv_response(self, xml_response):\n",
        "        \"\"\"Parse ArXiv API XML response\"\"\"\n",
        "        from bs4 import BeautifulSoup\n",
        "\n",
        "        soup = BeautifulSoup(xml_response, 'xml')\n",
        "        entries = soup.find_all('entry')\n",
        "\n",
        "        papers = []\n",
        "        for entry in entries:\n",
        "            paper = {\n",
        "                \"title\": entry.title.text.strip(),\n",
        "                \"authors\": [author.find('name').text for author in entry.find_all('author')],\n",
        "                \"abstract\": entry.summary.text.strip(),\n",
        "                \"published\": entry.published.text,\n",
        "                \"updated\": entry.updated.text,\n",
        "                \"arxiv_id\": entry.id.text.split('/')[-1],\n",
        "                \"pdf_url\": None\n",
        "            }\n",
        "\n",
        "            # Find PDF link\n",
        "            for link in entry.find_all('link'):\n",
        "                if link.get('title') == 'pdf':\n",
        "                    paper['pdf_url'] = link.get('href')\n",
        "                    break\n",
        "\n",
        "            papers.append(paper)\n",
        "\n",
        "        return papers\n",
        "\n",
        "    def fetch_paper_text(self, arxiv_id):\n",
        "        \"\"\"Fetch full paper text from Kaggle dataset\"\"\"\n",
        "        try:\n",
        "            # Load the Kaggle dataset\n",
        "            df = pd.read_csv('/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json',\n",
        "                           lines=True, nrows=100000)  # Load subset for demo\n",
        "\n",
        "            # Find the paper by arXiv ID\n",
        "            paper = df[df['id'] == arxiv_id].iloc[0]\n",
        "            return paper['abstract'] + \" \" + paper.get('title', '')  # Using abstract as proxy for full text\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "class RAGSummarizer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.processor = DocumentProcessor(config)\n",
        "        self.embedding_model = SentenceTransformer(config.embedding_model, device=config.device)\n",
        "        self.arxiv_search = ArXivSearch(config)\n",
        "\n",
        "        # Initialize the base model with LoRA\n",
        "        self.base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            \"meta-llama/Llama-3.2-1B\",\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        # Configure LoRA\n",
        "        lora_config = LoraConfig(\n",
        "            r=config.lora_rank,\n",
        "            lora_alpha=config.lora_alpha,\n",
        "            lora_dropout=config.lora_dropout,\n",
        "            target_modules=[\"q_proj\", \"v_proj\"],\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\"\n",
        "        )\n",
        "\n",
        "        # Apply LoRA to the base model\n",
        "        self.model = get_peft_model(self.base_model, lora_config)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
        "\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=config.dim,\n",
        "                nhead=config.heads,\n",
        "                dropout=config.dropout,\n",
        "                batch_first=True,\n",
        "                norm_first=True\n",
        "            ),\n",
        "            num_layers=config.layers\n",
        "        )\n",
        "\n",
        "        self.concept_selector = ConceptSelector(config.dim)\n",
        "        self.retriever = FAISSRetriever(config.embd_dim)\n",
        "\n",
        "    def encode_chunks(self, chunks):\n",
        "        return self.embedding_model.encode(\n",
        "            chunks,\n",
        "            convert_to_tensor=True,\n",
        "            device=self.config.device,\n",
        "            normalize_embeddings=True\n",
        "        )\n",
        "\n",
        "    def forward(self, document_text):\n",
        "        chunks = self.processor.chunk_document(document_text)\n",
        "        if not chunks:\n",
        "            return {\"chunks\": [], \"key_concepts\": [], \"document_embedding\": None}\n",
        "\n",
        "        chunk_embeds = self.encode_chunks(chunks)\n",
        "        encoded = self.transformer(chunk_embeds.unsqueeze(0))\n",
        "        concept_scores = self.concept_selector(encoded)\n",
        "\n",
        "        score_dist = torch.sigmoid(concept_scores)\n",
        "        top_k = min(max(int((score_dist > 0.65).sum().item()), 3),\n",
        "                   self.config.num_key_concepts, concept_scores.shape[-1])\n",
        "        top_indices = torch.topk(concept_scores, k=top_k, dim=-1).indices\n",
        "\n",
        "        return {\n",
        "            \"chunks\": chunks,\n",
        "            \"key_concepts\": [chunks[i] for i in top_indices[0]],\n",
        "            \"document_embedding\": encoded.mean(dim=1).squeeze().cpu().detach().numpy()\n",
        "        }\n",
        "\n",
        "    def generate_summary(self, text):\n",
        "        processed = self.forward(text)\n",
        "        if not processed[\"key_concepts\"]:\n",
        "            return {\"summary\": \"\", \"key_concepts\": []}\n",
        "\n",
        "        summary = self.generate_structured_summary(\n",
        "            processed[\"key_concepts\"],\n",
        "            max_tokens=self.config.summary_length,\n",
        "            temperature=self.config.temperature,\n",
        "            num_beams=self.config.num_beams\n",
        "        )\n",
        "\n",
        "        self.retriever.add(processed[\"document_embedding\"], {\n",
        "            \"summary\": summary,\n",
        "            \"concepts\": processed[\"key_concepts\"],\n",
        "            \"original_text\": text[:1000] + \"...\" if len(text) > 1000 else text\n",
        "        })\n",
        "\n",
        "        return {\n",
        "            \"summary\": summary,\n",
        "            \"key_concepts\": processed[\"key_concepts\"],\n",
        "            \"embedding\": processed[\"document_embedding\"],\n",
        "            \"scores\": self.evaluate_summary(summary, text)\n",
        "        }\n",
        "\n",
        "    def generate_structured_summary(self, concepts, max_tokens=250, temperature=0.5, num_beams=4):\n",
        "        concepts = [c[:512] for c in concepts]\n",
        "        prompt = f\"\"\"Generate a technical paper summary using these key points:\n",
        "\n",
        "Concepts:\n",
        "{chr(10).join(f'- {c}' for c in concepts)}\n",
        "\n",
        "Structure your summary with:\n",
        "1. Research objective and problem statement\n",
        "2. Methodology and technical approach\n",
        "3. Key findings and results\n",
        "4. Implications and future work\n",
        "\n",
        "Ensure:\n",
        "- Technical accuracy\n",
        "- Clear logical flow between paragraphs\n",
        "- Proper technical terminology\n",
        "- Concise but comprehensive coverage\n",
        "\n",
        "Summary:\"\"\"\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", max_length=4096, truncation=True).to(self.model.device)\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=0.95,\n",
        "            num_beams=num_beams,\n",
        "            repetition_penalty=1.15,\n",
        "            do_sample=True,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        full_summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return full_summary.split(\"Summary:\")[-1].strip()\n",
        "\n",
        "    def search_and_summarize(self, query):\n",
        "        \"\"\"Search ArXiv for papers and generate summaries\"\"\"\n",
        "        papers = self.arxiv_search.search_papers(query, self.config.max_retrieved_papers)\n",
        "\n",
        "        results = []\n",
        "        for paper in papers:\n",
        "            # Try to get full text from Kaggle dataset\n",
        "            full_text = self.arxiv_search.fetch_paper_text(paper['arxiv_id'])\n",
        "            if full_text is None:\n",
        "                full_text = paper['abstract']  # Fall back to abstract\n",
        "\n",
        "            result = self.generate_summary(full_text)\n",
        "            result['paper_info'] = paper\n",
        "            results.append(result)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def evaluate_summary(self, summary, reference):\n",
        "        \"\"\"Evaluate a single summary against reference text\"\"\"\n",
        "        return evaluate_summaries([summary], [reference])\n",
        "\n",
        "    def print_trainable_parameters(self):\n",
        "        \"\"\"Prints the number of trainable parameters in the model.\"\"\"\n",
        "        trainable_params = 0\n",
        "        all_param = 0\n",
        "        for _, param in self.model.named_parameters():\n",
        "            all_param += param.numel()\n",
        "            if param.requires_grad:\n",
        "                trainable_params += param.numel()\n",
        "        print(\n",
        "            f\"Trainable params: {trainable_params:,} || \"\n",
        "            f\"All params: {all_param:,} || \"\n",
        "            f\"Trainable%: {100 * trainable_params / all_param:.2f}%\"\n",
        "        )\n",
        "\n",
        "def evaluate_summaries(predictions, references):\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "    bertscore = evaluate.load(\"bertscore\")\n",
        "\n",
        "    rouge_scores = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
        "    bert_scores = bertscore.compute(predictions=predictions, references=references, lang=\"en\", model_type=\"roberta-large\")\n",
        "\n",
        "    content_sim = []\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        pred_ngrams = set(nltk.ngrams(pred.split(), 3))\n",
        "        ref_ngrams = set(nltk.ngrams(ref.split(), 3))\n",
        "        overlap = len(pred_ngrams & ref_ngrams) / len(ref_ngrams) if ref_ngrams else 0\n",
        "        content_sim.append(overlap)\n",
        "\n",
        "    return {\n",
        "        \"rouge1\": rouge_scores[\"rouge1\"],\n",
        "        \"rouge2\": rouge_scores[\"rouge2\"],\n",
        "        \"rougeL\": rouge_scores[\"rougeL\"],\n",
        "        \"bert_precision\": np.mean(bert_scores[\"precision\"]),\n",
        "        \"bert_recall\": np.mean(bert_scores[\"recall\"]),\n",
        "        \"bert_f1\": np.mean(bert_scores[\"f1\"]),\n",
        "        \"content_preservation\": np.mean(content_sim)\n",
        "    }\n",
        "\n",
        "def display_results(results):\n",
        "    \"\"\"Display search and summary results in notebook-friendly format\"\"\"\n",
        "    for i, result in enumerate(results):\n",
        "        display(Markdown(f\"### Paper {i+1}: {result['paper_info']['title']}\"))\n",
        "        display(Markdown(f\"**Authors**: {', '.join(result['paper_info']['authors'])}\"))\n",
        "        display(Markdown(f\"**Published**: {result['paper_info']['published']}\"))\n",
        "        display(Markdown(f\"**[PDF Link]({result['paper_info']['pdf_url']})**\"))\n",
        "\n",
        "        display(Markdown(\"#### Summary\"))\n",
        "        display(Markdown(result['summary']))\n",
        "\n",
        "        display(Markdown(\"#### Evaluation Scores\"))\n",
        "        scores = result['scores']\n",
        "        display(Markdown(f\"\"\"\n",
        "- ROUGE-1: {scores['rouge1']:.3f}\n",
        "- ROUGE-2: {scores['rouge2']:.3f}\n",
        "- ROUGE-L: {scores['rougeL']:.3f}\n",
        "- BERT F1: {scores['bert_f1']:.3f}\n",
        "- Content Preservation: {scores['content_preservation']:.3f}\n",
        "        \"\"\"))\n",
        "\n",
        "        display(Markdown(\"---\"))\n",
        "\n",
        "\n",
        "\n",
        "def run_search_pipeline(query=\"AI\", num_papers=None):\n",
        "    config = LCMConfig()\n",
        "    if num_papers:\n",
        "        config.max_retrieved_papers = num_papers\n",
        "\n",
        "    summarizer = RAGSummarizer(config).to(config.device)\n",
        "    summarizer.eval()\n",
        "\n",
        "    results = summarizer.search_and_summarize(query)\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_results = run_search_pipeline(\"quantum computing\")\n",
        "    for i, result in enumerate(test_results):\n",
        "        print(f\"\\nPaper {i+1}: {result['paper_info']['title']}\")\n",
        "        print(f\"Summary: {result['summary'][:200]}...\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DJr--R4OUADv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rag_pipeline import run_search_pipeline\n",
        "import gradio as gr\n",
        "import traceback\n",
        "\n",
        "def gradio_search(query, num_papers=5):\n",
        "    try:\n",
        "        # Limit num_papers to 5 maximum\n",
        "        num_papers = min(num_papers, 5)\n",
        "\n",
        "        results = run_search_pipeline(query, num_papers=num_papers)\n",
        "\n",
        "        output = []\n",
        "        for i, result in enumerate(results):\n",
        "            # Generate a random pastel color for each card header\n",
        "            hue = (i * 60) % 360  # Different hue for each card\n",
        "            bg_color = f\"hsla({hue}, 70%, 95%, 1)\"\n",
        "            accent_color = f\"hsla({hue}, 70%, 40%, 1)\"\n",
        "\n",
        "            output.append(f\"\"\"\n",
        "            <div class=\"paper-card\" style=\"\n",
        "                border: 1px solid #ddd;\n",
        "                border-radius: 12px;\n",
        "                padding: 0;\n",
        "                margin-bottom: 25px;\n",
        "                box-shadow: 0 4px 12px rgba(0,0,0,0.1);\n",
        "                font-family: system-ui, -apple-system, 'Segoe UI', Roboto, sans-serif;\n",
        "                max-width: 100%;\n",
        "                background-color: #ffffff;\n",
        "                overflow: hidden;\n",
        "                transition: transform 0.2s, box-shadow 0.2s;\n",
        "            \"\n",
        "            onmouseover=\"this.style.transform='translateY(-5px)';this.style.boxShadow='0 8px 24px rgba(0,0,0,0.15)';\"\n",
        "            onmouseout=\"this.style.transform='translateY(0)';this.style.boxShadow='0 4px 12px rgba(0,0,0,0.1)';\"\n",
        "            >\n",
        "                <div style=\"\n",
        "                    background-color: {bg_color};\n",
        "                    padding: 16px 20px;\n",
        "                    border-bottom: 1px solid #eee;\n",
        "                \">\n",
        "                    <h2 style=\"\n",
        "                        font-size: 1.4em;\n",
        "                        color: #333;\n",
        "                        margin: 0 0 10px 0;\n",
        "                        line-height: 1.3;\n",
        "                        font-weight: 600;\n",
        "                    \">{result['paper_info']['title']}</h2>\n",
        "                    <p style=\"\n",
        "                        color: #555;\n",
        "                        margin: 0;\n",
        "                        font-size: 0.95em;\n",
        "                    \"><strong>Authors:</strong> {', '.join(result['paper_info']['authors'][:3])}{', et al.' if len(result['paper_info']['authors']) > 3 else ''}</p>\n",
        "                </div>\n",
        "\n",
        "                <div style=\"padding: 20px;\">\n",
        "                    <div style=\"display: flex; flex-wrap: wrap; gap: 10px; margin-bottom: 20px;\">\n",
        "                        <a href=\"{result['paper_info']['pdf_url']}\" target=\"_blank\" style=\"\n",
        "                            display: inline-flex;\n",
        "                            align-items: center;\n",
        "                            gap: 6px;\n",
        "                            padding: 8px 16px;\n",
        "                            background-color: {accent_color};\n",
        "                            color: white;\n",
        "                            text-decoration: none;\n",
        "                            border-radius: 6px;\n",
        "                            font-size: 14px;\n",
        "                            font-weight: 500;\n",
        "                            transition: opacity 0.2s;\n",
        "                        \" onmouseover=\"this.style.opacity='0.9';\" onmouseout=\"this.style.opacity='1';\">\n",
        "                            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\">\n",
        "                                <path d=\"M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z\"></path>\n",
        "                                <polyline points=\"14 2 14 8 20 8\"></polyline>\n",
        "                                <line x1=\"16\" y1=\"13\" x2=\"8\" y2=\"13\"></line>\n",
        "                                <line x1=\"16\" y1=\"17\" x2=\"8\" y2=\"17\"></line>\n",
        "                                <polyline points=\"10 9 9 9 8 9\"></polyline>\n",
        "                            </svg>\n",
        "                            View PDF\n",
        "                        </a>\n",
        "                        <div style=\"\n",
        "                            display: inline-flex;\n",
        "                            align-items: center;\n",
        "                            gap: 6px;\n",
        "                            padding: 8px 16px;\n",
        "                            background-color: #f0f0f0;\n",
        "                            color: #666;\n",
        "                            border-radius: 6px;\n",
        "                            font-size: 14px;\n",
        "                        \">\n",
        "                            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"16\" height=\"16\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\">\n",
        "                                <rect x=\"3\" y=\"4\" width=\"18\" height=\"18\" rx=\"2\" ry=\"2\"></rect>\n",
        "                                <line x1=\"16\" y1=\"2\" x2=\"16\" y2=\"6\"></line>\n",
        "                                <line x1=\"8\" y1=\"2\" x2=\"8\" y2=\"6\"></line>\n",
        "                                <line x1=\"3\" y1=\"10\" x2=\"21\" y2=\"10\"></line>\n",
        "                            </svg>\n",
        "                            {result['paper_info']['published'][:10]}\n",
        "                        </div>\n",
        "                    </div>\n",
        "\n",
        "                    <div style=\"margin-top: 20px;\">\n",
        "                        <h3 style=\"\n",
        "                            font-size: 1.1em;\n",
        "                            color: #444;\n",
        "                            margin: 0 0 10px 0;\n",
        "                            padding-bottom: 8px;\n",
        "                            border-bottom: 2px solid {bg_color};\n",
        "                            display: inline-block;\n",
        "                        \">Summary</h3>\n",
        "                        <div style=\"\n",
        "                            background-color: #f9f9f9;\n",
        "                            padding: 16px;\n",
        "                            border-radius: 8px;\n",
        "                            color: #333;\n",
        "                            line-height: 1.5;\n",
        "                            font-size: 0.95em;\n",
        "                            margin-bottom: 20px;\n",
        "                        \">\n",
        "                            {result['summary']}\n",
        "                        </div>\n",
        "                    </div>\n",
        "\n",
        "                    <div class=\"key-concepts\">\n",
        "                        <h3 style=\"\n",
        "                            font-size: 1.1em;\n",
        "                            color: #444;\n",
        "                            margin: 0 0 10px 0;\n",
        "                            padding-bottom: 8px;\n",
        "                            border-bottom: 2px solid {bg_color};\n",
        "                            display: inline-block;\n",
        "                        \">Key Concepts</h3>\n",
        "                        <div class=\"concepts-grid\" style=\"\n",
        "                            display: grid;\n",
        "                            grid-template-columns: repeat(auto-fill, minmax(250px, 1fr));\n",
        "                            gap: 10px;\n",
        "                        \">\n",
        "                            {\"\".join(f'''\n",
        "                            <div style=\"\n",
        "                                background-color: #f5f5f5;\n",
        "                                border-left: 3px solid {accent_color};\n",
        "                                padding: 10px;\n",
        "                                border-radius: 6px;\n",
        "                                font-size: 0.9em;\n",
        "                                color: #555;\n",
        "                            \">\n",
        "                                {concept[:150]}{'...' if len(concept)>150 else ''}\n",
        "                            </div>\n",
        "                            ''' for concept in result['key_concepts'][:5])}\n",
        "                        </div>\n",
        "                    </div>\n",
        "                </div>\n",
        "            </div>\n",
        "            \"\"\")\n",
        "\n",
        "        # Add CSS for responsiveness\n",
        "        responsive_css = \"\"\"\n",
        "        <style>\n",
        "            @media (max-width: 768px) {\n",
        "                .concepts-grid {\n",
        "                    grid-template-columns: 1fr !important;\n",
        "                }\n",
        "\n",
        "                .paper-card h2 {\n",
        "                    font-size: 1.2em !important;\n",
        "                }\n",
        "\n",
        "                .paper-card {\n",
        "                    padding: 0 !important;\n",
        "                }\n",
        "            }\n",
        "        </style>\n",
        "        \"\"\"\n",
        "\n",
        "        return responsive_css + \"\".join(output)\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"\"\"\n",
        "        <div style=\"\n",
        "            border: 1px solid #f8d7da;\n",
        "            background-color: #fff5f5;\n",
        "            color: #721c24;\n",
        "            padding: 20px;\n",
        "            border-radius: 8px;\n",
        "            margin: 20px 0;\n",
        "            font-family: system-ui, -apple-system, 'Segoe UI', Roboto, sans-serif;\n",
        "        \">\n",
        "            <h3 style=\"margin-top: 0;\">‚ö†Ô∏è Error Occurred</h3>\n",
        "            <p>{str(e)}</p>\n",
        "            <details>\n",
        "                <summary style=\"cursor: pointer; color: #721c24; font-weight: bold; margin: 10px 0;\">\n",
        "                    Show technical details\n",
        "                </summary>\n",
        "                <pre style=\"\n",
        "                    background-color: #f8f8f8;\n",
        "                    padding: 15px;\n",
        "                    border-radius: 5px;\n",
        "                    overflow-x: auto;\n",
        "                    color: #333;\n",
        "                    font-size: 0.9em;\n",
        "                \">{traceback.format_exc()}</pre>\n",
        "            </details>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        return error_message\n",
        "\n",
        "# Custom CSS for Gradio interface\n",
        "custom_css = \"\"\"\n",
        ".gradio-container {\n",
        "    max-width: 900px !important;\n",
        "    margin-left: auto !important;\n",
        "    margin-right: auto !important;\n",
        "}\n",
        "\n",
        ".main-container {\n",
        "    padding: 0 !important;\n",
        "}\n",
        "\n",
        "/* Loading animation */\n",
        "@keyframes pulse {\n",
        "    0% { opacity: 0.6; }\n",
        "    50% { opacity: 1; }\n",
        "    100% { opacity: 0.6; }\n",
        "}\n",
        "\n",
        ".loading {\n",
        "    animation: pulse 1.5s infinite;\n",
        "}\n",
        "\n",
        "/* Responsive design */\n",
        "@media (max-width: 768px) {\n",
        "    .gradio-container {\n",
        "        padding: 10px !important;\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    with gr.Blocks(css=custom_css) as iface:\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            # üìö ArXiv IntelliSearch üìö\n",
        "\n",
        "            Get AI-generated summaries of the latest research papers from ArXiv.\n",
        "            Enter a topic and let our model find and summarize relevant papers.\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            query = gr.Textbox(\n",
        "                label=\"Research Topic\",\n",
        "                placeholder=\"Enter a topic (e.g., quantum mechanics, AI, climate change)\",\n",
        "                lines=1\n",
        "            )\n",
        "            num_papers = gr.Slider(\n",
        "                minimum=1,\n",
        "                maximum=5,\n",
        "                value=3,\n",
        "                step=1,\n",
        "                label=\"Number of Papers (max 5)\"\n",
        "            )\n",
        "\n",
        "        search_btn = gr.Button(\"Search Papers\", variant=\"primary\")\n",
        "\n",
        "        # Output area\n",
        "        output = gr.HTML(\n",
        "            label=\"Results\",\n",
        "            value=\"\"\"<div style=\"text-align: center; color: #666; padding: 30px;\">\n",
        "                Enter a research topic above and click \"Search Papers\" to get started\n",
        "            </div>\"\"\"\n",
        "        )\n",
        "\n",
        "        # Examples - reduced to just two key examples\n",
        "        gr.Examples(\n",
        "            examples=[\n",
        "                [\"large language models\", 3],\n",
        "                [\"quantum computing\", 2]\n",
        "            ],\n",
        "            inputs=[query, num_papers]\n",
        "        )\n",
        "\n",
        "        # Loading state\n",
        "        search_btn.click(\n",
        "            fn=lambda: \"\"\"<div style=\"text-align: center; padding: 40px; color: #666;\" class=\"loading\">\n",
        "                    <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"40\" height=\"40\" viewBox=\"0 0 24 24\" fill=\"none\"\n",
        "                         stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\"\n",
        "                         style=\"margin: 0 auto 20px auto; display: block;\">\n",
        "                        <path d=\"M21 12a9 9 0 1 1-6.219-8.56\"></path>\n",
        "                    </svg>\n",
        "                    <p>Searching and analyzing research papers...</p>\n",
        "                    <p style=\"font-size: 0.85em;\">This may take a minute</p>\n",
        "                </div>\"\"\",\n",
        "            outputs=output\n",
        "        ).then(\n",
        "            fn=gradio_search,\n",
        "            inputs=[query, num_papers],\n",
        "            outputs=output\n",
        "        )\n",
        "\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            ### How it works\n",
        "\n",
        "            This tool uses:\n",
        "            1. A specialized RAG pipeline to find relevant papers\n",
        "            2. LLM-based key concept extraction\n",
        "            3. Advanced summarization techniques to create concise research summaries\n",
        "\n",
        "            *Note: Summaries are AI-generated and should be used as a starting point for further research.*\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "    # Launch the interface\n",
        "    iface.launch(server_port=5000, share=True)"
      ],
      "metadata": {
        "id": "z8hMVIBnT_w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python gradio_app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-7zbF5so59m",
        "outputId": "823bbeb6-4453-4122-d4ac-a1d3a5b57aec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-30 05:33:05.505269: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1751261585.538644    1333 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1751261585.557435    1333 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-30 05:33:05.615292: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "* Running on local URL:  http://127.0.0.1:5000\n",
            "* Running on public URL: https://30b3a8b86c570f281e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 343kB/s]\n",
            "config.json: 100% 571/571 [00:00<00:00, 3.35MB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 40.5MB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 50.1MB/s]\n",
            "modules.json: 100% 349/349 [00:00<00:00, 2.60MB/s]\n",
            "config_sentence_transformers.json: 100% 116/116 [00:00<00:00, 930kB/s]\n",
            "README.md: 10.4kB [00:00, 25.5MB/s]\n",
            "sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 257kB/s]\n",
            "config.json: 100% 571/571 [00:00<00:00, 4.15MB/s]\n",
            "model.safetensors: 100% 438M/438M [00:10<00:00, 41.0MB/s]\n",
            "tokenizer_config.json: 100% 363/363 [00:00<00:00, 2.67MB/s]\n",
            "vocab.txt: 232kB [00:00, 14.0MB/s]\n",
            "tokenizer.json: 466kB [00:00, 13.6MB/s]\n",
            "special_tokens_map.json: 100% 239/239 [00:00<00:00, 1.92MB/s]\n",
            "config.json: 100% 190/190 [00:00<00:00, 1.55MB/s]\n",
            "config.json: 100% 843/843 [00:00<00:00, 5.71MB/s]\n",
            "model.safetensors: 100% 2.47G/2.47G [00:39<00:00, 62.1MB/s]\n",
            "generation_config.json: 100% 185/185 [00:00<00:00, 958kB/s]\n",
            "tokenizer_config.json: 100% 50.5k/50.5k [00:00<00:00, 66.6MB/s]\n",
            "tokenizer.json: 100% 9.09M/9.09M [00:01<00:00, 6.64MB/s]\n",
            "special_tokens_map.json: 100% 301/301 [00:00<00:00, 1.75MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Downloading builder script: 6.27kB [00:00, 15.3MB/s]\n",
            "Downloading builder script: 7.95kB [00:00, 12.1MB/s]\n",
            "tokenizer_config.json: 100% 25.0/25.0 [00:00<00:00, 139kB/s]\n",
            "config.json: 100% 482/482 [00:00<00:00, 3.37MB/s]\n",
            "vocab.json: 100% 899k/899k [00:00<00:00, 4.05MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 1.04MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 3.08MB/s]\n",
            "model.safetensors: 100% 1.42G/1.42G [01:39<00:00, 14.3MB/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Downloading builder script: 6.27kB [00:00, 16.2MB/s]\n",
            "Downloading builder script: 7.95kB [00:00, 24.5MB/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UjDA9Fu-ULa7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}